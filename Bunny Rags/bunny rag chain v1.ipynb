{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293b2ecf-7294-4d60-a7c5-9caa0008a78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- 1. GLOBAL SILENCING CONFIGURATION ---\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 2. LOGGING SETUP ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "noisy_loggers = [\"sentence_transformers\", \"transformers\", \"urllib3\", \"requests\", \"huggingface_hub\", \"filelock\", \"tqdm\"]\n",
    "for logger_name in noisy_loggers:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "from builder import CausalGraphBuilder\n",
    "from bunny_retriever import BunnyPathRetriever\n",
    "from explainer import CausalGraphExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5de2913d-e2bd-4f5f-85da-37f55a8974db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-29 18:05:03,473 - INFO - Loading graph from causal_math_graph_llm.json...\n",
      "\n",
      "============================================================\n",
      "USER QUERY: What happens when the circumcenter is on the side of the triangle\n",
      "============================================================\n",
      "\n",
      "--- TOP 15 NODES EXPLORED IN THE CAUSAL GRAPH ---\n",
      "Node Description                                             | Score\n",
      "---------------------------------------------------------------------------\n",
      "a triangle changes shape                                     | -0.0200\n",
      "squared integer                                              | 0.2448\n",
      "minor-closed property                                        | 0.2462\n",
      "Marangoni forces                                             | 0.3835\n",
      "atoms and molecules exist                                    | 0.3844\n",
      "\n",
      "--- EXPLORATION SUMMARY ---\n",
      "Total Nodes explored: 5\n",
      "Primary Anchor: a triangle changes shape\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from typing import List, Tuple, Dict, Any\n",
    "from builder import CausalGraphBuilder\n",
    "from bunny_retriever import BunnyPathRetriever\n",
    "\n",
    "# Setup logging to see the retrieval process\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BunnyRAGChain:\n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\", graph_path: str = \"causal_math_graph_llm.json\"):\n",
    "        \"\"\"\n",
    "        Initializes the Causal Graph components.\n",
    "        \"\"\"\n",
    "        self.builder = CausalGraphBuilder(model_name=model_name)\n",
    "        \n",
    "        # Load the existing knowledge graph\n",
    "        logger.info(f\"Loading graph from {graph_path}...\")\n",
    "        self.builder.load(graph_path)\n",
    "        \n",
    "        # Initialize the Bunny Retriever\n",
    "        self.retriever = BunnyPathRetriever(self.builder)\n",
    "        self.graph_path = graph_path\n",
    "\n",
    "    def explore_and_query(self, query: str):\n",
    "        \"\"\"\n",
    "        Executes the RAG chain and explicitly shows the top 15 nodes explored.\n",
    "        \"\"\"\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"USER QUERY: {query}\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "        # 1. RETRIEVAL STEP: Get top 15 nodes using Effective Resistance\n",
    "        # We use labda=0.1 to balance structural distance and semantic similarity\n",
    "        top_15_results = self.retriever.retrieve_nodes_part2(\n",
    "            query=query, \n",
    "            top_k=5, \n",
    "            labda=0.02, \n",
    "            json_path=self.graph_path\n",
    "        )\n",
    "\n",
    "        # 2. DISPLAY STEP: Show the nodes being explored\n",
    "        print(\"--- TOP 15 NODES EXPLORED IN THE CAUSAL GRAPH ---\")\n",
    "        print(f\"{'Node Description':<60} | {'Score'}\")\n",
    "        print(\"-\" * 75)\n",
    "        \n",
    "        explored_node_ids = []\n",
    "        for node_id, score in top_15_results:\n",
    "            # Get display text from builder's node_text map\n",
    "            display_text = self.builder.node_text.get(node_id, node_id)\n",
    "            # Truncate for clean table display\n",
    "            print(f\"{display_text[:58]:<60} | {score:.4f}\")\n",
    "            explored_node_ids.append(node_id)\n",
    "\n",
    "        # 3. CONTEXT BUILDING\n",
    "        # Here you would typically find paths between these nodes or \n",
    "        # pull raw text snippets (similar to your _get_context_for_path logic)\n",
    "        \n",
    "        print(f\"\\n--- EXPLORATION SUMMARY ---\")\n",
    "        print(f\"Total Nodes explored: {len(explored_node_ids)}\")\n",
    "        print(f\"Primary Anchor: {explored_node_ids[0] if explored_node_ids else 'None'}\")\n",
    "        \n",
    "        return explored_node_ids\n",
    "\n",
    "# --- Execution ---\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure causal_math_graph_llm.json is in your directory\n",
    "    chain = BunnyRAGChain()\n",
    "    \n",
    "    # Example Query\n",
    "    nodes = chain.explore_and_query(\"What happens when the circumcenter is on the side of the triangle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8ef2716-22ff-48ae-b499-0316ce240de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer # Add this\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        # Initialize ROUGE scorer for L (Longest Common Subsequence) and ROUGE-1/2\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        if \"No direct causal paths found\" in retrieved_context:\n",
    "            return 0.0, 0, 0, 0.0 # Added 0.0 for ROUGE\n",
    "            \n",
    "        # --- Existing Semantic Similarity ---\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        \n",
    "        # --- New ROUGE Calculation ---\n",
    "        # rougeL is particularly good for causal paths as it respects word order\n",
    "        scores = self.scorer.score(truth, retrieved_context)\n",
    "        rouge_l_f1 = scores['rougeL'].fmeasure\n",
    "        \n",
    "        # --- Updated Logic ---\n",
    "        recall = 1 if cosine_score > 0.7 else 0\n",
    "        precision = 1 if \"PATH 1:\" in retrieved_context else 0\n",
    "        \n",
    "        return cosine_score, recall, precision, rouge_l_f1\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        \n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l = self.calculate_metrics(item['context'], item['truth'])\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"ROUGE_L\": round(rouge_l, 4), # New metric\n",
    "                \"Recall\": recall,\n",
    "                \"Precision\": prec,\n",
    "                \"Status\": \"Success\" if (recall == 1 or rouge_l > 0.5) else \"Fail\"\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa18da6f-0824-45b6-ab0c-3aa978a0322c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SentenceTransformer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Execution ---\u001b[39;00m\n\u001b[32m      2\u001b[39m data_to_evaluate = [\n\u001b[32m      3\u001b[39m     {\n\u001b[32m      4\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mquery\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mWhat happens when the circumcenter is on the side of the triangle?\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m     }\n\u001b[32m     18\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m evaluator = \u001b[43mCausalRAGEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m df = evaluator.run_evaluation(data_to_evaluate)\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m### RAG Evaluation Results ###\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 5\u001b[39m, in \u001b[36mCausalRAGEvaluator.__init__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval_model = \u001b[43mSentenceTransformer\u001b[49m(\u001b[33m'\u001b[39m\u001b[33mall-mpnet-base-v2\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# Initialize ROUGE scorer for L (Longest Common Subsequence) and ROUGE-1/2\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mself\u001b[39m.scorer = rouge_scorer.RougeScorer([\u001b[33m'\u001b[39m\u001b[33mrouge1\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrougeL\u001b[39m\u001b[33m'\u001b[39m], use_stemmer=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'SentenceTransformer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Execution ---\n",
    "data_to_evaluate = [\n",
    "    {\n",
    "        \"query\": \"What happens when the circumcenter is on the side of the triangle?\",\n",
    "        \"context\": \"PATH 1: the circumcenter is located on the side of the triangle -> the triangle is acute\\nPATH 2: the circumcenter is located on the side of the triangle -> the angle opposite that side is a right angle\",\n",
    "        \"truth\": \"If the circumcenter is on a side, the angle opposite is a right angle and the triangle is a right triangle.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What influences the velocity of a Brownian particle?\",\n",
    "        \"context\": \"No direct causal paths found in the knowledge graph.\",\n",
    "        \"truth\": \"The velocity is influenced by temperature, thermal fluctuations, and fluid viscosity.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about surface tension and minimal surfaces.\",\n",
    "        \"context\": \"No direct causal paths found in the knowledge graph.\",\n",
    "        \"truth\": \"Surface tension causes soap films to form minimal surfaces that minimize surface area.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluator = CausalRAGEvaluator()\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "\n",
    "print(\"### RAG Evaluation Results ###\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8713ca1-d5c3-40dd-8993-1cbb4c63279f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self, json_file_path):\n",
    "        # Load the provided causal knowledge graph\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.knowledge_graph = json.load(f)\n",
    "        \n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    def _get_relevant_references(self, truth_text):\n",
    "        \"\"\"\n",
    "        In a real RAG, this represents the set R (ground truth references).\n",
    "        We will tokenize the truth or match it against KG nodes.\n",
    "        \"\"\"\n",
    "        # For this implementation, we treat the 'truth' string as the reference set R\n",
    "        return [truth_text.lower()]\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        if \"No direct causal paths found\" in retrieved_context:\n",
    "            return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # 1. Formal Context Recall (Reference Equation 1)\n",
    "        # Recall = (Count of retrieved items in Reference Set) / |Reference Set|\n",
    "        references = self._get_relevant_references(truth)\n",
    "        retrieved_items = [p.strip().lower() for p in retrieved_context.split('->')]\n",
    "        \n",
    "        # Indicator function: 1 if retrieved item is in the truth references\n",
    "        hits = sum(1 for item in retrieved_items if any(ref in item for ref in references))\n",
    "        context_recall = hits / len(references) if references else 0.0\n",
    "\n",
    "        # 2. Formal Context Precision (Reference Equation 2)\n",
    "        # Precision = (Sum of indicator values) / (Total retrieved items)\n",
    "        context_precision = hits / len(retrieved_items) if retrieved_items else 0.0\n",
    "\n",
    "        # 3. Semantic Similarity\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "        # 4. ROUGE-L\n",
    "        rouge_scores = self.scorer.score(truth, retrieved_context)\n",
    "        rouge_l = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        return similarity, context_recall, context_precision, rouge_l\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l = self.calculate_metrics(item['context'], item['truth'])\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"Context Recall\": round(recall, 4),\n",
    "                \"Context Precision\": round(prec, 4),\n",
    "                \"ROUGE_L\": round(rouge_l, 4),\n",
    "                \"Status\": \"Success\" if recall > 0.5 and rouge_l > 0.3 else \"Fail\"\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n",
    "# --- Execute with your data ---\n",
    "# Assuming 'causal_math_graph_llm.json' is in your directory\n",
    "evaluator = CausalRAGEvaluator('causal_math_graph_llm.json')\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "\n",
    "print(\"### Formalized RAG Evaluation Results ###\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b32bbd-4d1f-4678-a38e-f244cfa5dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "import itertools # For pairwise combinations\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self, json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.kg_data = json.load(f)\n",
    "        \n",
    "        # Extract node names and variants for formal Recall/Precision\n",
    "        self.all_valid_nodes = set(self.kg_data['nodes'].keys())\n",
    "        for variant_list in self.kg_data['variants'].values():\n",
    "            self.all_valid_nodes.update(variant_list)\n",
    "        \n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_diversity(self, retrieved_paths):\n",
    "        \"\"\"\n",
    "        Implements Diversity: distance(di, dj) = 1 - cos_sim(emb(di), emb(dj))\n",
    "        Returns the mean of all pairwise distances.\n",
    "        \"\"\"\n",
    "        if len(retrieved_paths) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Embed each path (d_i)\n",
    "        embeddings = self.eval_model.encode(retrieved_paths, convert_to_tensor=True)\n",
    "        distances = []\n",
    "\n",
    "        # Compare all i < j pairs\n",
    "        for i, j in itertools.combinations(range(len(retrieved_paths)), 2):\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "            # distance = 1 - similarity\n",
    "            distances.append(1 - sim)\n",
    "            \n",
    "        return sum(distances) / len(distances) if distances else 0.0\n",
    "\n",
    "    def _extract_nodes(self, text):\n",
    "        text_lower = text.lower()\n",
    "        return {node.lower() for node in self.all_valid_nodes if node.lower() in text_lower}\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        # Split context into individual paths for diversity and formal metrics\n",
    "        # We assume paths are separated by \"PATH\" or newlines\n",
    "        retrieved_paths = [p.strip() for p in retrieved_context.split('\\n') if \"PATH\" in p]\n",
    "        \n",
    "        if not retrieved_paths:\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # --- Formal Recall/Precision (per your Image equations) ---\n",
    "        R = self._extract_nodes(truth) # Reference set R\n",
    "        C = self._extract_nodes(retrieved_context) # Retrieved set Ci\n",
    "        \n",
    "        hits = sum(1 for node in C if node in R)\n",
    "        context_recall = hits / len(R) if R else 1.0 # Indicator function sum / |R|\n",
    "        context_precision = hits / len(C) if C else 0.0 # Indicator sum / retrieved count\n",
    "\n",
    "        # --- Diversity ---\n",
    "        diversity = self.calculate_diversity(retrieved_paths)\n",
    "\n",
    "        # --- Similarity & ROUGE ---\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        rouge_l = self.scorer.score(truth, retrieved_context)['rougeL'].fmeasure\n",
    "\n",
    "        return similarity, context_recall, context_precision, rouge_l, diversity\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l, div = self.calculate_metrics(item['context'], item['truth'])\n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"Recall\": round(recall, 4),\n",
    "                \"Precision\": round(prec, 4),\n",
    "                \"Diversity\": round(div, 4), # Higher = more varied info\n",
    "                \"ROUGE_L\": round(rouge_l, 4)\n",
    "            })\n",
    "        return pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c8c19d-660c-41ee-8c61-f6acec74d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize with your JSON\n",
    "evaluator = CausalRAGEvaluator('causal_math_graph_llm.json')\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d5972-eecf-42b7-8058-41462710281d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
