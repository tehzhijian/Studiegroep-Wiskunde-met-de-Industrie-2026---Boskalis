{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293b2ecf-7294-4d60-a7c5-9caa0008a78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- 1. GLOBAL SILENCING CONFIGURATION ---\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 2. LOGGING SETUP ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "noisy_loggers = [\"sentence_transformers\", \"transformers\", \"urllib3\", \"requests\", \"huggingface_hub\", \"filelock\", \"tqdm\"]\n",
    "for logger_name in noisy_loggers:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "# --- 3. IMPORTS ---\n",
    "try:\n",
    "    from causal_graph.builder import CausalGraphBuilder\n",
    "    from causal_graph.retriever import CausalPathRetriever\n",
    "    from causal_graph.explainer import CausalGraphExplainer\n",
    "except ImportError:\n",
    "    from builder import CausalGraphBuilder\n",
    "    from retriever import CausalPathRetriever\n",
    "    from explainer import CausalGraphExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d63050-58eb-428d-ac1e-42b8121ad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CausalRAGChain:\n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: The SentenceTransformer model. \n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.logger.info(f\"Initializing Causal Graph Builder with model: {model_name}...\")\n",
    "        self.builder = CausalGraphBuilder(\n",
    "            model_name=model_name, \n",
    "            normalize_nodes=True\n",
    "        )\n",
    "        self.retriever = None\n",
    "        self.documents = []  # NEW: Store original documents for context lookup\n",
    "\n",
    "    def load_graph_state(self, filepath: str):\n",
    "        \"\"\"Loads an existing graph state (nodes/edges) from JSON.\"\"\"\n",
    "        self.logger.info(f\"Loading graph state from {filepath}...\")\n",
    "        if os.path.exists(filepath):\n",
    "            success = self.builder.load(filepath)\n",
    "            if success:\n",
    "                self.logger.info(f\"Graph loaded successfully: {self.builder.get_graph().number_of_nodes()} nodes.\")\n",
    "            else:\n",
    "                self.logger.error(\"Failed to parse graph file. Starting with empty graph.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Graph file not found: {filepath}. Starting with empty graph.\")\n",
    "        \n",
    "        self.retriever = CausalPathRetriever(self.builder)\n",
    "\n",
    "    def save_graph_state(self, filepath: str):\n",
    "        \"\"\"Saves the current graph state to JSON.\"\"\"\n",
    "        self.builder.save(filepath)\n",
    "\n",
    "    def ingest_wiki_knowledge(self, json_path: str, limit: int = None, auto_save_path: str = None):\n",
    "        \"\"\"\n",
    "        Loads wiki json, stores raw text for retrieval, and builds the graph.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Ingesting knowledge from {json_path}...\")\n",
    "        \n",
    "        if not os.path.exists(json_path):\n",
    "            self.logger.error(f\"Knowledge base file not found: {json_path}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract text\n",
    "            self.documents = [] # Reset documents\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if 'raw_text' in item:\n",
    "                        self.documents.append(item['raw_text'])\n",
    "            \n",
    "            if not self.documents:\n",
    "                self.logger.warning(\"No 'raw_text' fields found in JSON.\")\n",
    "                return\n",
    "\n",
    "            # Apply limit if specified\n",
    "            if limit:\n",
    "                self.documents = self.documents[:limit]\n",
    "                self.logger.info(f\"Limiting ingestion to first {limit} documents.\")\n",
    "\n",
    "            self.logger.info(f\"Indexing {len(self.documents)} documents into the graph...\")\n",
    "            \n",
    "            # Index documents into graph\n",
    "            self.builder.index_documents(self.documents, show_progress=False)\n",
    "            \n",
    "            self.logger.info(f\"Ingestion complete. Graph size: {self.builder.get_graph().number_of_nodes()} nodes.\")\n",
    "            self.retriever = CausalPathRetriever(self.builder)\n",
    "            \n",
    "            if auto_save_path:\n",
    "                self.save_graph_state(auto_save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during ingestion: {e}\")\n",
    "\n",
    "    def _get_context_for_path(self, path: list[str], window_size: int = 300) -> str:\n",
    "        \"\"\"\n",
    "        NEW: Finds the snippet in the source documents that contains the nodes in the path.\n",
    "        This provides the 'narrative' context surrounding the causal arrow.\n",
    "        \"\"\"\n",
    "        best_snippet = \"\"\n",
    "        max_matches = 0\n",
    "        \n",
    "        # Convert path nodes to a set of keywords (lowercase for matching)\n",
    "        path_keywords = [node.lower() for node in path]\n",
    "        \n",
    "        # Heuristic: Search documents for sentences containing the Cause and Effect\n",
    "        for doc in self.documents:\n",
    "            doc_lower = doc.lower()\n",
    "            \n",
    "            # Count how many path nodes appear in this document\n",
    "            matches = sum(1 for keyword in path_keywords if keyword in doc_lower)\n",
    "            \n",
    "            if matches >= 2 and matches > max_matches:\n",
    "                # If we find a document containing multiple nodes from the chain, extract context\n",
    "                max_matches = matches\n",
    "                \n",
    "                # Find the position of the first keyword occurrence\n",
    "                first_pos = doc_lower.find(path_keywords[0])\n",
    "                if first_pos != -1:\n",
    "                    start = max(0, first_pos - window_size)\n",
    "                    end = min(len(doc), first_pos + window_size * 2)\n",
    "                    best_snippet = f\"...{doc[start:end]}...\"\n",
    "        \n",
    "        return best_snippet if best_snippet else \"Context not found in source text.\"\n",
    "\n",
    "    def run(self, query: str):\n",
    "        \"\"\"Runs the retrieval chain with Context Enrichment.\"\"\"\n",
    "        if not self.retriever:\n",
    "            self.retriever = CausalPathRetriever(self.builder)\n",
    "            \n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        \n",
    "        # 1. Retrieve Causal Paths (The \"Skeleton\" of the answer)\n",
    "        paths = self.retriever.retrieve_paths(\n",
    "            query, \n",
    "            max_paths=5, \n",
    "            min_path_length=2, \n",
    "            max_path_length=4\n",
    "        )\n",
    "        \n",
    "        # 2. Retrieve Source Context (The \"Flesh\" of the answer)\n",
    "        # We look up the original text for each path found\n",
    "        context_blocks = []\n",
    "        for i, path in enumerate(paths):\n",
    "            arrow_chain = \" -> \".join(path)\n",
    "            source_snippet = self._get_context_for_path(path)\n",
    "            \n",
    "            block = (\n",
    "                f\"PATH {i+1}: {arrow_chain}\\n\"\n",
    "                f\"SOURCE CONTEXT: {source_snippet}\\n\"\n",
    "            )\n",
    "            context_blocks.append(block)\n",
    "        \n",
    "        paths_context_text = \"\\n\".join(context_blocks)\n",
    "        \n",
    "        if not paths_context_text:\n",
    "            paths_context_text = \"No direct causal paths found in the knowledge graph.\"\n",
    "            \n",
    "        # 3. Enhanced Prompt\n",
    "        prompt = f\"\"\"You are a Causal AI Expert. \n",
    "Using the provided Causal Paths and their Source Context, write a coherent, detailed answer.\n",
    "Do not just list the paths; weave them into a narrative explanation.\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "=== RETRIEVED CAUSAL EVIDENCE ===\n",
    "{paths_context_text}\n",
    "=================================\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"paths\": paths, \n",
    "            \"context_text\": paths_context_text, # Return context for debugging\n",
    "            \"final_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d19544e-8ef1-43d1-aa8b-a06401890b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-29 15:17:39,328 - INFO - Initializing Causal Graph Builder with model: all-mpnet-base-v2...\n",
      "2026-01-29 15:17:42,743 - INFO - Loading graph state from causal_math_graph_state_llm.json...\n",
      "2026-01-29 15:17:43,282 - INFO - Graph loaded successfully: 4 nodes.\n",
      "2026-01-29 15:17:43,285 - INFO - Ingesting knowledge from wiki_math_knowledge_base_api.json...\n",
      "2026-01-29 15:17:43,302 - INFO - Limiting ingestion to first 20 documents.\n",
      "2026-01-29 15:17:43,304 - INFO - Indexing 20 documents into the graph...\n",
      "2026-01-29 15:17:44,150 - INFO - Processed batch 4/4: found 0 causal relationships\n",
      "2026-01-29 15:17:44,153 - INFO - Indexing complete: 20 documents processed\n",
      "2026-01-29 15:17:44,154 - INFO - Added 0 new nodes and 0 new relationships to graph\n",
      "2026-01-29 15:17:44,155 - INFO - Graph now has 4 nodes and 3 edges\n",
      "2026-01-29 15:17:44,156 - INFO - Ingestion complete. Graph size: 4 nodes.\n",
      "\n",
      "Processing 3 queries... (Saving results to rag_output_with_context.txt)\n",
      "\n",
      "Processing query: What happens when the circumcenter is on the side of the triangle?\n",
      "Finished Query 1\n",
      "\n",
      "Processing query: What influences the velocity of a Brownian particle?\n",
      "Finished Query 2\n",
      "\n",
      "Processing query: Tell me about surface tension and minimal surfaces.\n",
      "Finished Query 3\n",
      "\n",
      "Done! Check 'rag_output_with_context.txt' to see the paths linked with their original text.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    GRAPH_STATE_FILE = \"causal_math_graph_state_llm.json\"\n",
    "    WIKI_KB_FILE = \"wiki_math_knowledge_base_api.json\"\n",
    "    OUTPUT_FILE = \"rag_output_with_context.txt\"\n",
    "    \n",
    "    # 1. Initialize Chain\n",
    "    # We use the same model as before\n",
    "    chain = CausalRAGChain(model_name=\"all-mpnet-base-v2\")\n",
    "    \n",
    "    # 2. Load Existing Graph State\n",
    "    # This loads the nodes and edges you've already built\n",
    "    chain.load_graph_state(GRAPH_STATE_FILE)\n",
    "    \n",
    "    # 3. Ingest Data (CRITICAL STEP)\n",
    "    # Even if the graph is loaded, we MUST run this to populate 'self.documents'\n",
    "    # so the chain can look up the original text context.\n",
    "    # We use limit=20 to match your previous test; remove 'limit' for full run.\n",
    "    chain.ingest_wiki_knowledge(WIKI_KB_FILE, limit=20, auto_save_path=GRAPH_STATE_FILE)\n",
    "    \n",
    "    # 4. Define Queries\n",
    "    queries = [\n",
    "        'What happens when the circumcenter is on the side of the triangle?',\n",
    "        \"What influences the velocity of a Brownian particle?\",\n",
    "        \"Tell me about surface tension and minimal surfaces.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(queries)} queries... (Saving results to {OUTPUT_FILE})\")\n",
    "    \n",
    "    # 5. Run and Save\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== CAUSAL RAG RESULTS WITH SOURCE CONTEXT ===\\n\\n\")\n",
    "        \n",
    "        for i, q in enumerate(queries, 1):\n",
    "            # The run() method now returns 'context_text' containing the source snippets\n",
    "            result = chain.run(q)\n",
    "            \n",
    "            output_block = []\n",
    "            output_block.append(f\"QUERY {i}: {result['query']}\")\n",
    "            output_block.append(\"-\" * 40)\n",
    "            \n",
    "            # Display the Retrieved Evidence (Paths + Source Text)\n",
    "            if result.get('context_text'):\n",
    "                output_block.append(\"RETRIEVED EVIDENCE & CONTEXT:\")\n",
    "                output_block.append(result['context_text'])\n",
    "            else:\n",
    "                output_block.append(\"  [INFO]: No evidence found.\")\n",
    "            \n",
    "            output_block.append(\"-\" * 40)\n",
    "            \n",
    "            # Display the Final Prompt (What you would send to an LLM)\n",
    "            output_block.append(\"FINAL GENERATED PROMPT:\")\n",
    "            output_block.append(result['final_prompt'])\n",
    "            \n",
    "            output_block.append(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "            # Write to file\n",
    "            full_text = \"\\n\".join(output_block)\n",
    "            f.write(full_text)\n",
    "            f.flush()\n",
    "            \n",
    "            print(f\"Finished Query {i}\")\n",
    "\n",
    "    print(f\"\\nDone! Check '{OUTPUT_FILE}' to see the paths linked with their original text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8ef2716-22ff-48ae-b499-0316ce240de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer # Add this\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self):\n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        # Initialize ROUGE scorer for L (Longest Common Subsequence) and ROUGE-1/2\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        if \"No direct causal paths found\" in retrieved_context:\n",
    "            return 0.0, 0, 0, 0.0 # Added 0.0 for ROUGE\n",
    "            \n",
    "        # --- Existing Semantic Similarity ---\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        cosine_score = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        \n",
    "        # --- New ROUGE Calculation ---\n",
    "        # rougeL is particularly good for causal paths as it respects word order\n",
    "        scores = self.scorer.score(truth, retrieved_context)\n",
    "        rouge_l_f1 = scores['rougeL'].fmeasure\n",
    "        \n",
    "        # --- Updated Logic ---\n",
    "        recall = 1 if cosine_score > 0.7 else 0\n",
    "        precision = 1 if \"PATH 1:\" in retrieved_context else 0\n",
    "        \n",
    "        return cosine_score, recall, precision, rouge_l_f1\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        \n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l = self.calculate_metrics(item['context'], item['truth'])\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"ROUGE_L\": round(rouge_l, 4), # New metric\n",
    "                \"Recall\": recall,\n",
    "                \"Precision\": prec,\n",
    "                \"Status\": \"Success\" if (recall == 1 or rouge_l > 0.5) else \"Fail\"\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fa18da6f-0824-45b6-ab0c-3aa978a0322c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-29 15:36:11,992 - INFO - Using default tokenizer.\n",
      "### RAG Evaluation Results ###\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Query</th>\n",
       "      <th>Similarity</th>\n",
       "      <th>ROUGE_L</th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What happens when the circumcenter is on the s...</td>\n",
       "      <td>0.7294</td>\n",
       "      <td>0.4138</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Success</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What influences the velocity of a Brownian par...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tell me about surface tension and minimal surf...</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Fail</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Query  Similarity  ROUGE_L  \\\n",
       "0  What happens when the circumcenter is on the s...      0.7294   0.4138   \n",
       "1  What influences the velocity of a Brownian par...      0.0000   0.0000   \n",
       "2  Tell me about surface tension and minimal surf...      0.0000   0.0000   \n",
       "\n",
       "   Recall  Precision   Status  \n",
       "0       1          1  Success  \n",
       "1       0          0     Fail  \n",
       "2       0          0     Fail  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# --- Execution ---\n",
    "data_to_evaluate = [\n",
    "    {\n",
    "        \"query\": \"What happens when the circumcenter is on the side of the triangle?\",\n",
    "        \"context\": \"PATH 1: the circumcenter is located on the side of the triangle -> the triangle is acute\\nPATH 2: the circumcenter is located on the side of the triangle -> the angle opposite that side is a right angle\",\n",
    "        \"truth\": \"If the circumcenter is on a side, the angle opposite is a right angle and the triangle is a right triangle.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"What influences the velocity of a Brownian particle?\",\n",
    "        \"context\": \"No direct causal paths found in the knowledge graph.\",\n",
    "        \"truth\": \"The velocity is influenced by temperature, thermal fluctuations, and fluid viscosity.\"\n",
    "    },\n",
    "    {\n",
    "        \"query\": \"Tell me about surface tension and minimal surfaces.\",\n",
    "        \"context\": \"No direct causal paths found in the knowledge graph.\",\n",
    "        \"truth\": \"Surface tension causes soap films to form minimal surfaces that minimize surface area.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "evaluator = CausalRAGEvaluator()\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "\n",
    "print(\"### RAG Evaluation Results ###\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a8713ca1-d5c3-40dd-8993-1cbb4c63279f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-29 15:54:05,818 - INFO - Using default tokenizer.\n",
      "### Formalized RAG Evaluation Results ###\n",
      "                                                             Query  Similarity  Context Recall  Context Precision  ROUGE_L Status\n",
      "What happens when the circumcenter is on the side of the triangle?      0.7294             0.0                0.0   0.4138   Fail\n",
      "              What influences the velocity of a Brownian particle?      0.0000             0.0                0.0   0.0000   Fail\n",
      "               Tell me about surface tension and minimal surfaces.      0.0000             0.0                0.0   0.0000   Fail\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self, json_file_path):\n",
    "        # Load the provided causal knowledge graph\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.knowledge_graph = json.load(f)\n",
    "        \n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    def _get_relevant_references(self, truth_text):\n",
    "        \"\"\"\n",
    "        In a real RAG, this represents the set R (ground truth references).\n",
    "        We will tokenize the truth or match it against KG nodes.\n",
    "        \"\"\"\n",
    "        # For this implementation, we treat the 'truth' string as the reference set R\n",
    "        return [truth_text.lower()]\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        if \"No direct causal paths found\" in retrieved_context:\n",
    "            return 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # 1. Formal Context Recall (Reference Equation 1)\n",
    "        # Recall = (Count of retrieved items in Reference Set) / |Reference Set|\n",
    "        references = self._get_relevant_references(truth)\n",
    "        retrieved_items = [p.strip().lower() for p in retrieved_context.split('->')]\n",
    "        \n",
    "        # Indicator function: 1 if retrieved item is in the truth references\n",
    "        hits = sum(1 for item in retrieved_items if any(ref in item for ref in references))\n",
    "        context_recall = hits / len(references) if references else 0.0\n",
    "\n",
    "        # 2. Formal Context Precision (Reference Equation 2)\n",
    "        # Precision = (Sum of indicator values) / (Total retrieved items)\n",
    "        context_precision = hits / len(retrieved_items) if retrieved_items else 0.0\n",
    "\n",
    "        # 3. Semantic Similarity\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "\n",
    "        # 4. ROUGE-L\n",
    "        rouge_scores = self.scorer.score(truth, retrieved_context)\n",
    "        rouge_l = rouge_scores['rougeL'].fmeasure\n",
    "\n",
    "        return similarity, context_recall, context_precision, rouge_l\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l = self.calculate_metrics(item['context'], item['truth'])\n",
    "            \n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"Context Recall\": round(recall, 4),\n",
    "                \"Context Precision\": round(prec, 4),\n",
    "                \"ROUGE_L\": round(rouge_l, 4),\n",
    "                \"Status\": \"Success\" if recall > 0.5 and rouge_l > 0.3 else \"Fail\"\n",
    "            })\n",
    "            \n",
    "        return pd.DataFrame(evaluation_results)\n",
    "\n",
    "# --- Execute with your data ---\n",
    "# Assuming 'causal_math_graph_llm.json' is in your directory\n",
    "evaluator = CausalRAGEvaluator('causal_math_graph_llm.json')\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "\n",
    "print(\"### Formalized RAG Evaluation Results ###\")\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5b32bbd-4d1f-4678-a38e-f244cfa5dc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from rouge_score import rouge_scorer\n",
    "import itertools # For pairwise combinations\n",
    "\n",
    "class CausalRAGEvaluator:\n",
    "    def __init__(self, json_file_path):\n",
    "        with open(json_file_path, 'r') as f:\n",
    "            self.kg_data = json.load(f)\n",
    "        \n",
    "        # Extract node names and variants for formal Recall/Precision\n",
    "        self.all_valid_nodes = set(self.kg_data['nodes'].keys())\n",
    "        for variant_list in self.kg_data['variants'].values():\n",
    "            self.all_valid_nodes.update(variant_list)\n",
    "        \n",
    "        self.eval_model = SentenceTransformer('all-mpnet-base-v2')\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "\n",
    "    def calculate_diversity(self, retrieved_paths):\n",
    "        \"\"\"\n",
    "        Implements Diversity: distance(di, dj) = 1 - cos_sim(emb(di), emb(dj))\n",
    "        Returns the mean of all pairwise distances.\n",
    "        \"\"\"\n",
    "        if len(retrieved_paths) < 2:\n",
    "            return 0.0\n",
    "        \n",
    "        # Embed each path (d_i)\n",
    "        embeddings = self.eval_model.encode(retrieved_paths, convert_to_tensor=True)\n",
    "        distances = []\n",
    "\n",
    "        # Compare all i < j pairs\n",
    "        for i, j in itertools.combinations(range(len(retrieved_paths)), 2):\n",
    "            sim = util.cos_sim(embeddings[i], embeddings[j]).item()\n",
    "            # distance = 1 - similarity\n",
    "            distances.append(1 - sim)\n",
    "            \n",
    "        return sum(distances) / len(distances) if distances else 0.0\n",
    "\n",
    "    def _extract_nodes(self, text):\n",
    "        text_lower = text.lower()\n",
    "        return {node.lower() for node in self.all_valid_nodes if node.lower() in text_lower}\n",
    "\n",
    "    def calculate_metrics(self, retrieved_context, truth):\n",
    "        # Split context into individual paths for diversity and formal metrics\n",
    "        # We assume paths are separated by \"PATH\" or newlines\n",
    "        retrieved_paths = [p.strip() for p in retrieved_context.split('\\n') if \"PATH\" in p]\n",
    "        \n",
    "        if not retrieved_paths:\n",
    "            return 0.0, 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        # --- Formal Recall/Precision (per your Image equations) ---\n",
    "        R = self._extract_nodes(truth) # Reference set R\n",
    "        C = self._extract_nodes(retrieved_context) # Retrieved set Ci\n",
    "        \n",
    "        hits = sum(1 for node in C if node in R)\n",
    "        context_recall = hits / len(R) if R else 1.0 # Indicator function sum / |R|\n",
    "        context_precision = hits / len(C) if C else 0.0 # Indicator sum / retrieved count\n",
    "\n",
    "        # --- Diversity ---\n",
    "        diversity = self.calculate_diversity(retrieved_paths)\n",
    "\n",
    "        # --- Similarity & ROUGE ---\n",
    "        embeddings = self.eval_model.encode([retrieved_context, truth], convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(embeddings[0], embeddings[1]).item()\n",
    "        rouge_l = self.scorer.score(truth, retrieved_context)['rougeL'].fmeasure\n",
    "\n",
    "        return similarity, context_recall, context_precision, rouge_l, diversity\n",
    "\n",
    "    def run_evaluation(self, results_data):\n",
    "        evaluation_results = []\n",
    "        for item in results_data:\n",
    "            sim, recall, prec, rouge_l, div = self.calculate_metrics(item['context'], item['truth'])\n",
    "            evaluation_results.append({\n",
    "                \"Query\": item['query'],\n",
    "                \"Similarity\": round(sim, 4),\n",
    "                \"Recall\": round(recall, 4),\n",
    "                \"Precision\": round(prec, 4),\n",
    "                \"Diversity\": round(div, 4), # Higher = more varied info\n",
    "                \"ROUGE_L\": round(rouge_l, 4)\n",
    "            })\n",
    "        return pd.DataFrame(evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f0c8c19d-660c-41ee-8c61-f6acec74d768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-29 16:09:24,628 - INFO - Using default tokenizer.\n",
      "                                                             Query  Similarity  Recall  Precision  Diversity  ROUGE_L\n",
      "What happens when the circumcenter is on the side of the triangle?      0.7294     1.0        0.0     0.1503   0.4138\n",
      "              What influences the velocity of a Brownian particle?      0.0000     0.0        0.0     0.0000   0.0000\n",
      "               Tell me about surface tension and minimal surfaces.      0.0000     0.0        0.0     0.0000   0.0000\n"
     ]
    }
   ],
   "source": [
    "# Initialize with your JSON\n",
    "evaluator = CausalRAGEvaluator('causal_math_graph_llm.json')\n",
    "df = evaluator.run_evaluation(data_to_evaluate)\n",
    "print(df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0d5972-eecf-42b7-8058-41462710281d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
