{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa28f5ca-6d53-428d-b11c-076c3b464ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- 1. GLOBAL SILENCING CONFIGURATION ---\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 2. LOGGING SETUP ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "noisy_loggers = [\"sentence_transformers\", \"transformers\", \"urllib3\", \"requests\", \"huggingface_hub\", \"filelock\", \"tqdm\"]\n",
    "for logger_name in noisy_loggers:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "# --- 3. IMPORTS ---\n",
    "try:\n",
    "    from causal_graph.builder import CausalGraphBuilder\n",
    "    from causal_graph.retriever import CausalPathRetriever\n",
    "    from causal_graph.explainer import CausalGraphExplainer\n",
    "except ImportError:\n",
    "    from builder import CausalGraphBuilder\n",
    "    from retriever import CausalPathRetriever\n",
    "    from explainer import CausalGraphExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e8f172-1512-435f-960a-a2cd74ab43ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalRAGChain:\n",
    "    \"\"\"\n",
    "    A RAG Chain that uses a Causal Graph to ground answers in cause-effect relationships.\n",
    "    \"\"\"\n",
    "    def __init__(self, json_path: str, graphml_path: str = None):\n",
    "        print(f\"Initializing Causal Graph Chain...\")\n",
    "        \n",
    "        # 1. Initialize the Builder\n",
    "        # We use 'all-MiniLM-L6-v2' as defined in your builder.py defaults\n",
    "        self.builder = CausalGraphBuilder(model_name=\"all-MiniLM-L6-v2\")\n",
    "        \n",
    "        # 2. Load the Graph Data\n",
    "        # The JSON file contains the critical 'nodes', 'variants', and 'edges' structure\n",
    "        # required by the builder.py load() method.\n",
    "        print(f\"Loading semantic data from {json_path}...\")\n",
    "        success = self.builder.load(json_path)\n",
    "        if not success:\n",
    "            raise ValueError(f\"Failed to load graph data from {json_path}\")\n",
    "            \n",
    "        # Optional: If you wanted to enforce specific topology from GraphML,\n",
    "        # you could overlay it here, but the JSON provided already contains\n",
    "        # the edge list identical to the GraphML.\n",
    "        \n",
    "        # 3. Initialize the Retriever\n",
    "        self.retriever = CausalPathRetriever(self.builder)\n",
    "        print(\"Chain initialized successfully.\")\n",
    "\n",
    "    def run(self, query: str, context_window: int = 5) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the chain: Query -> Retrieve Causal Paths -> Synthesize Answer\n",
    "        \"\"\"\n",
    "        print(f\"\\nProcessing Query: '{query}'\")\n",
    "        \n",
    "        # --- Step A: Semantic & Structural Retrieval ---\n",
    "        # 1. Identify the core concept using vector similarity\n",
    "        # This uses the embeddings created by the builder\n",
    "        relevant_nodes = self.retriever.retrieve_nodes(query, top_k=3)\n",
    "        if not relevant_nodes:\n",
    "            return {\"answer\": \"I couldn't find any relevant concepts in the causal graph.\"}\n",
    "        \n",
    "        top_node_id, score = relevant_nodes[0]\n",
    "        print(f\"Found anchor concept: '{top_node_id}' (Score: {score:.2f})\")\n",
    "\n",
    "        # 2. Retrieve Causal Paths (The \"Guide\")\n",
    "        # This traverses the graph to find multi-hop causal chains\n",
    "        paths = self.retriever.retrieve_paths(\n",
    "            query, \n",
    "            max_paths=context_window, \n",
    "            min_path_length=2\n",
    "        )\n",
    "        \n",
    "        # --- Step B: Context Formatting ---\n",
    "        # Convert paths into a readable \"Causal Context\" string\n",
    "        context_str = \"Known Causal Relationships:\\n\"\n",
    "        path_details = []\n",
    "        \n",
    "        if paths:\n",
    "            for i, path in enumerate(paths):\n",
    "                # path is a list of strings, e.g., [\"smoking\", \"lung cancer\"]\n",
    "                chain_str = \" -> \".join(path)\n",
    "                context_str += f\"{i+1}. {chain_str}\\n\"\n",
    "                path_details.append(path)\n",
    "        else:\n",
    "            # Fallback if no full paths found: look for direct neighbors\n",
    "            context_str += \"Direct relationships found:\\n\"\n",
    "            descendants = self.retriever._get_descendants(top_node_id, max_hops=1)\n",
    "            ancestors = self.retriever._get_ancestors(top_node_id, max_hops=1)\n",
    "            \n",
    "            for item in descendants:\n",
    "                context_str += f\"- {top_node_id} causes {item}\\n\"\n",
    "            for item in ancestors:\n",
    "                context_str += f\"- {item} causes {top_node_id}\\n\"\n",
    "\n",
    "        # --- Step C: Generation (Simulated LLM) ---\n",
    "        # In a real app, you would pass 'prompt' to GPT-4/Gemini/etc.\n",
    "        prompt = f\"\"\"\n",
    "        You are a Causal Reasoning Assistant. Use the provided Knowledge Graph context to answer the question.\n",
    "        Do not invent information outside the graph.\n",
    "        \n",
    "        Context:\n",
    "        {context_str}\n",
    "        \n",
    "        Question: {query}\n",
    "        \n",
    "        Answer:\n",
    "        \"\"\"\n",
    "        \n",
    "        # For demonstration, we return the constructed prompt and paths\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"anchor_node\": top_node_id,\n",
    "            \"retrieved_paths\": path_details,\n",
    "            \"llm_prompt\": prompt,\n",
    "            \"raw_context\": context_str\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab8c1a8-a3e3-4214-bb9a-1dec7575fd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your uploaded files\n",
    "json_file = 'causal_math_graph_llm.json'\n",
    "\n",
    "# Instantiate the Chain\n",
    "rag_chain = CausalRAGChain(json_path=json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed6257f-6a3d-469a-92ca-c585ee13898b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9679e2-6649-482a-b6b4-ca17ea5120cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
