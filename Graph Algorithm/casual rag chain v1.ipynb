{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "293b2ecf-7294-4d60-a7c5-9caa0008a78f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import warnings\n",
    "from typing import Dict, Any\n",
    "\n",
    "# --- 1. GLOBAL SILENCING CONFIGURATION ---\n",
    "os.environ[\"TQDM_DISABLE\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"error\"\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- 2. LOGGING SETUP ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, \n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[logging.StreamHandler(sys.stdout)]\n",
    ")\n",
    "\n",
    "noisy_loggers = [\"sentence_transformers\", \"transformers\", \"urllib3\", \"requests\", \"huggingface_hub\", \"filelock\", \"tqdm\"]\n",
    "for logger_name in noisy_loggers:\n",
    "    logging.getLogger(logger_name).setLevel(logging.ERROR)\n",
    "\n",
    "# --- 3. IMPORTS ---\n",
    "try:\n",
    "    from causal_graph.builder import CausalGraphBuilder\n",
    "    from causal_graph.retriever import CausalPathRetriever\n",
    "    from causal_graph.explainer import CausalGraphExplainer\n",
    "except ImportError:\n",
    "    from builder import CausalGraphBuilder\n",
    "    from retriever import CausalPathRetriever\n",
    "    from explainer import CausalGraphExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60d63050-58eb-428d-ac1e-42b8121ad111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class CausalRAGChain:\n",
    "    def __init__(self, model_name: str = \"all-mpnet-base-v2\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: The SentenceTransformer model. \n",
    "        \"\"\"\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "        self.logger.info(f\"Initializing Causal Graph Builder with model: {model_name}...\")\n",
    "        self.builder = CausalGraphBuilder(\n",
    "            model_name=model_name, \n",
    "            normalize_nodes=True\n",
    "        )\n",
    "        self.retriever = None\n",
    "        self.documents = []  # NEW: Store original documents for context lookup\n",
    "\n",
    "    def load_graph_state(self, filepath: str):\n",
    "        \"\"\"Loads an existing graph state (nodes/edges) from JSON.\"\"\"\n",
    "        self.logger.info(f\"Loading graph state from {filepath}...\")\n",
    "        if os.path.exists(filepath):\n",
    "            success = self.builder.load(filepath)\n",
    "            if success:\n",
    "                self.logger.info(f\"Graph loaded successfully: {self.builder.get_graph().number_of_nodes()} nodes.\")\n",
    "            else:\n",
    "                self.logger.error(\"Failed to parse graph file. Starting with empty graph.\")\n",
    "        else:\n",
    "            self.logger.warning(f\"Graph file not found: {filepath}. Starting with empty graph.\")\n",
    "        \n",
    "        self.retriever = CausalPathRetriever(self.builder)\n",
    "\n",
    "    def save_graph_state(self, filepath: str):\n",
    "        \"\"\"Saves the current graph state to JSON.\"\"\"\n",
    "        self.builder.save(filepath)\n",
    "\n",
    "    def ingest_wiki_knowledge(self, json_path: str, limit: int = None, auto_save_path: str = None):\n",
    "        \"\"\"\n",
    "        Loads wiki json, stores raw text for retrieval, and builds the graph.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Ingesting knowledge from {json_path}...\")\n",
    "        \n",
    "        if not os.path.exists(json_path):\n",
    "            self.logger.error(f\"Knowledge base file not found: {json_path}\")\n",
    "            return\n",
    "\n",
    "        try:\n",
    "            with open(json_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Extract text\n",
    "            self.documents = [] # Reset documents\n",
    "            if isinstance(data, list):\n",
    "                for item in data:\n",
    "                    if 'raw_text' in item:\n",
    "                        self.documents.append(item['raw_text'])\n",
    "            \n",
    "            if not self.documents:\n",
    "                self.logger.warning(\"No 'raw_text' fields found in JSON.\")\n",
    "                return\n",
    "\n",
    "            # Apply limit if specified\n",
    "            if limit:\n",
    "                self.documents = self.documents[:limit]\n",
    "                self.logger.info(f\"Limiting ingestion to first {limit} documents.\")\n",
    "\n",
    "            self.logger.info(f\"Indexing {len(self.documents)} documents into the graph...\")\n",
    "            \n",
    "            # Index documents into graph\n",
    "            self.builder.index_documents(self.documents, show_progress=False)\n",
    "            \n",
    "            self.logger.info(f\"Ingestion complete. Graph size: {self.builder.get_graph().number_of_nodes()} nodes.\")\n",
    "            self.retriever = CausalPathRetriever(self.builder)\n",
    "            \n",
    "            if auto_save_path:\n",
    "                self.save_graph_state(auto_save_path)\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error during ingestion: {e}\")\n",
    "\n",
    "    def _get_context_for_path(self, path: list[str], window_size: int = 300) -> str:\n",
    "        \"\"\"\n",
    "        NEW: Finds the snippet in the source documents that contains the nodes in the path.\n",
    "        This provides the 'narrative' context surrounding the causal arrow.\n",
    "        \"\"\"\n",
    "        best_snippet = \"\"\n",
    "        max_matches = 0\n",
    "        \n",
    "        # Convert path nodes to a set of keywords (lowercase for matching)\n",
    "        path_keywords = [node.lower() for node in path]\n",
    "        \n",
    "        # Heuristic: Search documents for sentences containing the Cause and Effect\n",
    "        for doc in self.documents:\n",
    "            doc_lower = doc.lower()\n",
    "            \n",
    "            # Count how many path nodes appear in this document\n",
    "            matches = sum(1 for keyword in path_keywords if keyword in doc_lower)\n",
    "            \n",
    "            if matches >= 2 and matches > max_matches:\n",
    "                # If we find a document containing multiple nodes from the chain, extract context\n",
    "                max_matches = matches\n",
    "                \n",
    "                # Find the position of the first keyword occurrence\n",
    "                first_pos = doc_lower.find(path_keywords[0])\n",
    "                if first_pos != -1:\n",
    "                    start = max(0, first_pos - window_size)\n",
    "                    end = min(len(doc), first_pos + window_size * 2)\n",
    "                    best_snippet = f\"...{doc[start:end]}...\"\n",
    "        \n",
    "        return best_snippet if best_snippet else \"Context not found in source text.\"\n",
    "\n",
    "    def run(self, query: str):\n",
    "        \"\"\"Runs the retrieval chain with Context Enrichment.\"\"\"\n",
    "        if not self.retriever:\n",
    "            self.retriever = CausalPathRetriever(self.builder)\n",
    "            \n",
    "        print(f\"\\nProcessing query: {query}\")\n",
    "        \n",
    "        # 1. Retrieve Causal Paths (The \"Skeleton\" of the answer)\n",
    "        paths = self.retriever.retrieve_paths(\n",
    "            query, \n",
    "            max_paths=5, \n",
    "            min_path_length=2, \n",
    "            max_path_length=4\n",
    "        )\n",
    "        \n",
    "        # 2. Retrieve Source Context (The \"Flesh\" of the answer)\n",
    "        # We look up the original text for each path found\n",
    "        context_blocks = []\n",
    "        for i, path in enumerate(paths):\n",
    "            arrow_chain = \" -> \".join(path)\n",
    "            source_snippet = self._get_context_for_path(path)\n",
    "            \n",
    "            block = (\n",
    "                f\"PATH {i+1}: {arrow_chain}\\n\"\n",
    "                f\"SOURCE CONTEXT: {source_snippet}\\n\"\n",
    "            )\n",
    "            context_blocks.append(block)\n",
    "        \n",
    "        paths_context_text = \"\\n\".join(context_blocks)\n",
    "        \n",
    "        if not paths_context_text:\n",
    "            paths_context_text = \"No direct causal paths found in the knowledge graph.\"\n",
    "            \n",
    "        # 3. Enhanced Prompt\n",
    "        prompt = f\"\"\"You are a Causal AI Expert. \n",
    "Using the provided Causal Paths and their Source Context, write a coherent, detailed answer.\n",
    "Do not just list the paths; weave them into a narrative explanation.\n",
    "\n",
    "USER QUERY: {query}\n",
    "\n",
    "=== RETRIEVED CAUSAL EVIDENCE ===\n",
    "{paths_context_text}\n",
    "=================================\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        return {\n",
    "            \"query\": query,\n",
    "            \"paths\": paths, \n",
    "            \"context_text\": paths_context_text, # Return context for debugging\n",
    "            \"final_prompt\": prompt\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d19544e-8ef1-43d1-aa8b-a06401890b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 13:46:54,767 - INFO - Initializing Causal Graph Builder with model: all-mpnet-base-v2...\n",
      "2026-01-28 13:46:55,214 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,224 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,330 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,342 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,459 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config_sentence_transformers.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,468 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/config_sentence_transformers.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,574 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/README.md \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,583 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/README.md \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,688 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/modules.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,698 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/modules.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,811 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/sentence_bert_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:55,820 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/sentence_bert_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:55,926 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/adapter_config.json \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-28 13:46:56,033 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:56,043 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/config.json \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c113d7c73e8f49238d8a3572b6601dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/199 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-28 13:46:56,479 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:56,488 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:56,593 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/tokenizer_config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:56,603 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/tokenizer_config.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:56,718 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2/tree/main/additional_chat_templates?recursive=false&expand=false \"HTTP/1.1 404 Not Found\"\n",
      "2026-01-28 13:46:56,829 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2/tree/main?recursive=true&expand=false \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:57,008 - INFO - HTTP Request: HEAD https://huggingface.co/sentence-transformers/all-mpnet-base-v2/resolve/main/1_Pooling/config.json \"HTTP/1.1 307 Temporary Redirect\"\n",
      "2026-01-28 13:46:57,017 - INFO - HTTP Request: HEAD https://huggingface.co/api/resolve-cache/models/sentence-transformers/all-mpnet-base-v2/e8c3b32edf5434bc2275fc9bab85f82640a19130/1_Pooling%2Fconfig.json \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:57,124 - INFO - HTTP Request: GET https://huggingface.co/api/models/sentence-transformers/all-mpnet-base-v2 \"HTTP/1.1 200 OK\"\n",
      "2026-01-28 13:46:57,132 - INFO - Loading graph state from causal_math_graph_state_llm.json...\n",
      "2026-01-28 13:46:57,340 - INFO - Graph loaded successfully: 4 nodes.\n",
      "2026-01-28 13:46:57,341 - INFO - Ingesting knowledge from wiki_math_knowledge_base_api.json...\n",
      "2026-01-28 13:46:57,350 - INFO - Limiting ingestion to first 20 documents.\n",
      "2026-01-28 13:46:57,351 - INFO - Indexing 20 documents into the graph...\n",
      "2026-01-28 13:46:57,604 - INFO - Processed batch 4/4: found 0 causal relationships\n",
      "2026-01-28 13:46:57,605 - INFO - Indexing complete: 20 documents processed\n",
      "2026-01-28 13:46:57,607 - INFO - Added 0 new nodes and 0 new relationships to graph\n",
      "2026-01-28 13:46:57,608 - INFO - Graph now has 4 nodes and 3 edges\n",
      "2026-01-28 13:46:57,608 - INFO - Ingestion complete. Graph size: 4 nodes.\n",
      "\n",
      "Processing 3 queries... (Saving results to rag_output_with_context.txt)\n",
      "\n",
      "Processing query: What happens when the circumcenter is on the side of the triangle?\n",
      "Finished Query 1\n",
      "\n",
      "Processing query: What influences the velocity of a Brownian particle?\n",
      "Finished Query 2\n",
      "\n",
      "Processing query: Tell me about surface tension and minimal surfaces.\n",
      "Finished Query 3\n",
      "\n",
      "Done! Check 'rag_output_with_context.txt' to see the paths linked with their original text.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import os\n",
    "    \n",
    "    # --- Configuration ---\n",
    "    GRAPH_STATE_FILE = \"causal_math_graph_state_llm.json\"\n",
    "    WIKI_KB_FILE = \"wiki_math_knowledge_base_api.json\"\n",
    "    OUTPUT_FILE = \"rag_output_with_context.txt\"\n",
    "    \n",
    "    # 1. Initialize Chain\n",
    "    # We use the same model as before\n",
    "    chain = CausalRAGChain(model_name=\"all-mpnet-base-v2\")\n",
    "    \n",
    "    # 2. Load Existing Graph State\n",
    "    # This loads the nodes and edges you've already built\n",
    "    chain.load_graph_state(GRAPH_STATE_FILE)\n",
    "    \n",
    "    # 3. Ingest Data (CRITICAL STEP)\n",
    "    # Even if the graph is loaded, we MUST run this to populate 'self.documents'\n",
    "    # so the chain can look up the original text context.\n",
    "    # We use limit=20 to match your previous test; remove 'limit' for full run.\n",
    "    chain.ingest_wiki_knowledge(WIKI_KB_FILE, limit=20, auto_save_path=GRAPH_STATE_FILE)\n",
    "    \n",
    "    # 4. Define Queries\n",
    "    queries = [\n",
    "        'What happens when the circumcenter is on the side of the triangle?',\n",
    "        \"What influences the velocity of a Brownian particle?\",\n",
    "        \"Tell me about surface tension and minimal surfaces.\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"\\nProcessing {len(queries)} queries... (Saving results to {OUTPUT_FILE})\")\n",
    "    \n",
    "    # 5. Run and Save\n",
    "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"=== CAUSAL RAG RESULTS WITH SOURCE CONTEXT ===\\n\\n\")\n",
    "        \n",
    "        for i, q in enumerate(queries, 1):\n",
    "            # The run() method now returns 'context_text' containing the source snippets\n",
    "            result = chain.run(q)\n",
    "            \n",
    "            output_block = []\n",
    "            output_block.append(f\"QUERY {i}: {result['query']}\")\n",
    "            output_block.append(\"-\" * 40)\n",
    "            \n",
    "            # Display the Retrieved Evidence (Paths + Source Text)\n",
    "            if result.get('context_text'):\n",
    "                output_block.append(\"RETRIEVED EVIDENCE & CONTEXT:\")\n",
    "                output_block.append(result['context_text'])\n",
    "            else:\n",
    "                output_block.append(\"  [INFO]: No evidence found.\")\n",
    "            \n",
    "            output_block.append(\"-\" * 40)\n",
    "            \n",
    "            # Display the Final Prompt (What you would send to an LLM)\n",
    "            output_block.append(\"FINAL GENERATED PROMPT:\")\n",
    "            output_block.append(result['final_prompt'])\n",
    "            \n",
    "            output_block.append(\"=\" * 60 + \"\\n\")\n",
    "            \n",
    "            # Write to file\n",
    "            full_text = \"\\n\".join(output_block)\n",
    "            f.write(full_text)\n",
    "            f.flush()\n",
    "            \n",
    "            print(f\"Finished Query {i}\")\n",
    "\n",
    "    print(f\"\\nDone! Check '{OUTPUT_FILE}' to see the paths linked with their original text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f845078-abec-4790-a900-0ce029ef76b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
